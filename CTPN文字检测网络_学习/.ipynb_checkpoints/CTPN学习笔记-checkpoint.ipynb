{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考资料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "01: https://cloud.tencent.com/developer/article/1152494 场景文本检测—CTPN算法介绍  \n",
    "02: https://blog.csdn.net/sinat_33486980/article/details/81099093 faster R-CNN中anchors 的生成过程（generate_anchors源码解析）  \n",
    "03: https://blog.csdn.net/shenxiaolu1984/article/details/51152614 【目标检测】Faster RCNN算法详解  \n",
    "04: https://zhuanlan.zhihu.com/p/34757009 <font color=\"#FF0000\">场景文字检测—CTPN原理与实现</font>  \n",
    "05: https://www.jianshu.com/p/027e9399e699 与CPTN（文字识别网络）作斗争的记录  \n",
    "06: https://www.zhihu.com/question/265345106/answer/294410307 目标检测中region proposal的作用？  \n",
    "07: https://blog.csdn.net/u011436429/article/details/80279536 ROI Pooling原理及实现  \n",
    "08: https://zhuanlan.zhihu.com/p/31426458 <font color=\"#FF0000\">一文读懂Faster RCNN</font>  \n",
    "09: https://zhuanlan.zhihu.com/p/43534801 <font color=\"#FF0000\">一文读懂CRNN+CTC文字识别</font>  \n",
    "10: https://blog.csdn.net/zchang81/article/details/78873347 CTPN - 自然场景文本检测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "01: https://github.com/xiaomaxiao/keras_ocr xiaomaxiao/keras_ocr  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 论文地址"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "01: https://arxiv.org/abs/1609.03605 Detecting Text in Natural Image with Connectionist Text Proposal Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 场景文字检测—CTPN笔记"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTPN简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于复杂场景的文字识别，首先要定位文字的位置，即文字检测。这一直是一个研究热点。  \n",
    "CTPN是在ECCV 2016提出的一种文字检测算法。CTPN结合CNN与LSTM深度网络，能有效的检测出复杂场景的横向分布的文字，是目前比较好的文字检测算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTPN网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络使用keras框架实现\n",
    "\n",
    "假设输入 N Images：  \n",
    "\n",
    "1: 首先VGG提取特征，获得大小为 NxHxWxC 的 conv5 feature map  \n",
    "\n",
    "2: 之后在conv5上做 3×3 的滑动窗口，即每个点都结合周围 3×3 区域特征获得一个长度为 3×3×C 的特征向量。输出 NxHxWx9C 的 feature map，该特征显然只有CNN学习到的空间特征  \n",
    "\n",
    "3: 再将这个feature map进行Reshape, NxHxWx9C -> (NH)xWx9C  \n",
    "\n",
    "4: 然后以 Batch=NH 且最大时间长度 T=W 的数据流输入双向LSTM，学习每一行的序列特征。双向LSTM输出 \n",
    "(NH)xWx256  \n",
    "\n",
    "5: 再经Reshape恢复形状 (NH)xWx256 -> NxHxWx256 该特征既包含空间特征，也包含了LSTM学习到的序列特征 \n",
    "  \n",
    "6: 然后经过“FC”卷积层，变为 NxHxWx256 的特征, “FC”卷积层为核为 1x1 的卷积只会改变 feature map 的厚度  \n",
    "\n",
    "7: 最后经过类似Faster R-CNN的RPN网络，获得text proposals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers, Input\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 骨干网络模块函数\n",
    "骨干网络为VGG16，使用VGG16对图片进行特征提取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_base(input_shape, pretrain_weights_path):\n",
    "    \"\"\"\n",
    "    骨干网络块，使用VGG16底部的卷积层用来对图形进行特征提取\n",
    "    需要使用预训练模型，请下载vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
    "    \"\"\"\n",
    "    base_model = VGG16(weights=None, include_top=False, input_shape=input_shape)\n",
    "    base_model.load_weights(pretrain_weights_path)\n",
    "    return base_model.input, base_model.get_layer('block5_conv3').output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reshape特征图到以宽为时间步的形式 NxHxWx9C -> (NH)xWx9C \n",
    "构建把feature map进行Reshape, NxHxWx9C -> (NH)xWx9C 的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_to_time_series(input_tensor):\n",
    "    \"\"\"\n",
    "    把输入tensor NxHxWx9C reshape 到 (NH)xWx9C\n",
    "    \"\"\"\n",
    "    tshape = tf.shape(input_tensor)\n",
    "    output_tensor = tf.reshape(input_tensor, [tshape[0]*tshape[1], tshape[2], tshape[3]])\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reshape特征图到高宽通道的形式 (NH)xWx256 -> NxHxWx256\n",
    "构建Reshape恢复形状 (NH)xWx256 -> NxHxWx256 的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_to_back(input_tensor_list):\n",
    "    \"\"\"\n",
    "    把输入blstm_tensor (NH)xWx256 通过rpn_conv_tensor的形状 reshape 到 NxHxWx256\n",
    "    \"\"\"\n",
    "    blstm, rpn_conv = input_tensor_list\n",
    "    rshape = tf.shape(rpn_conv)\n",
    "    output_tensor = tf.reshape(blstm, [rshape[0], rshape[1], rshape[2], -1])\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reshape特征图到PRN的形式 NxHxWx20 -> Nx(10HW)x2\n",
    "构建Reshape到RPN结构 NxHxWx20 -> Nx(10HW)x2 的函数  \n",
    "NxHxWx20 的意思1是已featrue map每个像素中心坐标生成10个候选框，每个候选框有预测两个类别(前景、背景)  \n",
    "NxHxWx20 的意思2是已featrue map每个像素中心坐标生成10个候选框，每个候选框有回归2个便宜量(基于每个候选框center_y的偏移量、基于每个候选框h的偏移量)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_to_rpn(input_tensor):\n",
    "    \"\"\"\n",
    "    把输入tensor NxHxWx20 reshape 到 Nx(10HW)x2\n",
    "    \"\"\"\n",
    "    tshape = tf.shape(input_tensor)\n",
    "    output_tensor = tf.reshape(input_tensor, [tshape[0], -1, 2])\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建完整网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, None, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, None, None, 6 1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, None, None, 6 36928       block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, None, None, 6 0           block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, None, None, 1 73856       block1_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, None, None, 1 147584      block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, None, None, 1 0           block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, None, None, 2 295168      block2_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)           (None, None, None, 2 590080      block3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)           (None, None, None, 2 590080      block3_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, None, None, 2 0           block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)           (None, None, None, 5 1180160     block3_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)           (None, None, None, 5 2359808     block4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)           (None, None, None, 5 2359808     block4_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, None, None, 5 0           block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)           (None, None, None, 5 2359808     block4_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)           (None, None, None, 5 2359808     block5_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)           (None, None, None, 5 2359808     block5_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "rpn_conv (Conv2D)               (None, None, None, 4 21238272    block5_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape2ts (Lambda)             (None, None, 4608)   0           rpn_conv[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "blstm (Bidirectional)           (None, None, 256)    3638016     reshape2ts[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape2back (Lambda)           (None, None, None, 2 0           blstm[0][0]                      \n",
      "                                                                 rpn_conv[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "fcnn (Conv2D)                   (None, None, None, 5 131584      reshape2back[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "rpn_class (Conv2D)              (None, None, None, 2 10260       fcnn[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "rpn_regress (Conv2D)            (None, None, None, 2 10260       fcnn[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "rpn_class_reshape (Lambda)      (None, None, 2)      0           rpn_class[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "rpn_regress_reshape (Lambda)    (None, None, 2)      0           rpn_regress[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 39,743,080\n",
      "Trainable params: 39,743,080\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def cptn_model(input_shape, pretrain_weights_path):\n",
    "    # 输入(None, None, 3) 输出(None/16， None/16, 3) 通过骨干网络进行特征提取 向下取整\n",
    "    input_tensor, vgg_tensor = nn_base(input_shape, pretrain_weights_path)\n",
    "    \n",
    "    # 使得每个点都结合周围(3, 3)的区域特征\n",
    "    # H*W*C -> H*W*9C\n",
    "    rpn_conv = layers.Conv2D(\n",
    "        512*9, (3, 3), padding='same', \n",
    "        activation='relu', name='rpn_conv')(vgg_tensor)\n",
    "    # 此步骤应是对上层网络输出的特征图进行reshape\n",
    "    # 使特征图的shape变成(NH)*W*C，以(NH)为批次，W为时间步进行学习\n",
    "    rpn_conv_reshape = layers.Lambda(\n",
    "        reshape_to_time_series, \n",
    "        output_shape=(None, 512*9),\n",
    "        name='reshape2ts')(rpn_conv)\n",
    "    # 此步骤是一个双向LSTM，单个反向输出完整的时间步特征图形状为(NH)*W*128\n",
    "    # 有两个方向会各自得到各自的特征图，按通道堆叠两个特征图 最终得到的特征图形状(NH)*W*256\n",
    "    blstm = layers.Bidirectional(\n",
    "        layers.GRU(128, return_sequences=True),\n",
    "        name='blstm')(rpn_conv_reshape)\n",
    "    # blstm张量的形状是(NH)*W*256 需要通过rpn_conv的形状来辅助恢复到-> N*H*W*256\n",
    "    # 以便后面的卷积操作\n",
    "    blstm_reshape = layers.Lambda(\n",
    "        reshape_to_back,\n",
    "        output_shape=(None, None, 256),\n",
    "        name='reshape2back')([blstm, rpn_conv])\n",
    "    # 经过“FC”卷积层，变为 N*H*W*512 的特征\n",
    "    fcnn = layers.Conv2D(\n",
    "        512, (1, 1), padding='same',\n",
    "        activation='relu', name='fcnn')(blstm_reshape)\n",
    "    # 通过卷积提取每个框中是否是前景还是背景的变量\n",
    "    clas = layers.Conv2D(\n",
    "        10*2, (1, 1), padding='same',\n",
    "        activation='linear', name='rpn_class')(fcnn)\n",
    "    # 将输出类别预测张量形状改变为N*HW10*2\n",
    "    clas = layers.Lambda(\n",
    "        reshape_to_rpn, output_shape=(None, 2),\n",
    "        name='rpn_class_reshape')(clas)\n",
    "    # 通过卷积提取每个框的的中心y坐标和高度两个值，也是每个框2个变量\n",
    "    regr = layers.Conv2D(\n",
    "        10*2, (1, 1), padding='same',\n",
    "        activation='linear', name='rpn_regress')(fcnn)\n",
    "    # 将输出框坐标预测张量形状改变为N*HW10*2\n",
    "    regr = layers.Lambda(\n",
    "        reshape_to_rpn, output_shape=(None, 2), \n",
    "        name='rpn_regress_reshape')(regr)\n",
    "    \n",
    "    model = Model(input_tensor, [clas, regr])\n",
    "    return model\n",
    "\n",
    "model = cptn_model((None, None, 3), 'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建RPN的两个损失函数\n",
    "对于边框偏移量回归使用 smooth L1 loss\n",
    "对于前背景分类使用 crossentropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rpn_loss_regr(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    smooth L1 loss\n",
    "  \n",
    "    y_ture [1][HXWX10][3] (class1,regr2) \n",
    "    y_pred [1][HXWX10][2] (reger2)\n",
    "    class的值为-1, 0， 1; \n",
    "    regr的值为Vc, Vh为预测的anchor和IOU最大的gtbox之间center_y和h的偏移量\n",
    "    \"\"\"\n",
    "    sigma = 9.0\n",
    "    clas = y_true[0, :, 0]\n",
    "    regr = y_true[0, :, 1:3]\n",
    "    # 使用标签的clas类别来选出最合适的anchor的偏移量进行损失函数计算\n",
    "    regr_keep = tf.where(tf.equal(clas, 1))[:, 0]\n",
    "    regr_true = tf.gather(regr, regr_keep)\n",
    "    regr_pred = tf.gather(y_pred[0], regr_keep)\n",
    "    diff = tf.abs(regr_true - regr_pred)\n",
    "    less_one = tf.cast(tf.less(diff, 1.0/sigma), 'float32')\n",
    "    loss = less_one * 0.5 * diff**2 * sigma + tf.abs(1 - less_one) * (diff - 0.5 / sigma)\n",
    "    # 求出每个可能的候选框的loss\n",
    "    loss = K.sum(loss, axis=1)\n",
    "    # 如果没有求出loss那么loss置零，有loss的话算出这个样本的候选框loss的均值\n",
    "    return K.switch(tf.size(loss) > 0, K.mean(loss), tf.constant(0.0)) \n",
    "\n",
    "\n",
    "def rpn_loss_clas(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    softmax loss\n",
    "    \n",
    "    y_true [1][HXWX10][1] class 不是one-hot编码\n",
    "    y_pred [1][HXWX10][2] class \n",
    "    \"\"\" \n",
    "    y_true = y_true[0, :, 0]\n",
    "    # 选出正负样本，过滤掉无关样本，也就是-1类的框直接忽略了，只留下0类（背景），1类（前景）两类框\n",
    "    clas_keep = tf.where(tf.not_equal(y_true, -1))[:, 0] \n",
    "    clas_true = tf.gather(y_true, clas_keep)\n",
    "    clas_pred = tf.gather(y_pred[0], clas_keep)\n",
    "    clas_true = tf.cast(clas_true, 'int64')\n",
    "    # 求每个标签和对应预测值的交叉熵误差（每个候选框）\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=clas_true, logits=clas_pred) \n",
    "    # 如果没有求出loss那么loss置零，有loss的话算出这个样本的候选框前背景loss的均值\n",
    "    return K.switch(tf.size(loss) > 0, K.clip(K.mean(loss), 0, 10), K.constant(0.0)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RPN标签计算，包含anchor计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算对应特征图的基础候选框\n",
    "1. 因为骨干网络使用的是VGG16，所以骨干网络提取特征后输出的特征图的h,w是原图的16分之1\n",
    "2. 所以特征图一个像素对应原图16x16的区域，候选框就是以这个区域的中心来生成，候选框的坐标是在原图上的\n",
    "3. 设定特征图每个像素中心生成10个不同大小的基于原图的候选框\n",
    "4. 假设特征图大小为(30, 20)那么候选框个数就是30x20x10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基础候选框生成器\n",
    "基础候选框是通过特征图计算出来的，不是预测出来的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_anchor(featuresize, scale):\n",
    "    # 每个候选点预测10个候选框，这个是候选框基于原图的高度，有10种不同的高度\n",
    "    heights = [11, 16, 23, 33, 48, 68, 97, 139, 198, 283]\n",
    "    # 每个候选点预测10个候选框，这个是候选框基于原图的宽度，对于文字预测，宽度相等\n",
    "    widths  = [16, 16, 16, 16, 16, 16, 16,  16,  16,  16] \n",
    "\n",
    "    heights = np.array(heights).reshape(len(heights), 1) # 把数据格式转换为numpy\n",
    "    widths  = np.array(widths).reshape(len(widths), 1)\n",
    "    \n",
    "    # 因为使用的是VGG16的特征图，选取的特征图和原图大小关系刚好是16倍，这里是在选取一个基本anchor\n",
    "    base_anchor = np.array([0, 0, 15, 15]) \n",
    "    # 通过基本anchor求出候选框中心坐标\n",
    "    cx = (base_anchor[0] + base_anchor[2]) / 2.0 # 宽度一半得到x中心坐标\n",
    "    cy = (base_anchor[1] + base_anchor[3]) / 2.0 # 高度一半得到y中心坐标\n",
    "\n",
    "    # 求出在基础候选点的10个候选框坐标\n",
    "    x1 = cx - widths / 2.0\n",
    "    y1 = cy - heights / 2.0\n",
    "    x2 = cx + widths / 2.0\n",
    "    y2 = cy + heights / 2.0\n",
    "    base_anchor = np.hstack((x1, y1, x2, y2)) # shape=(10, 4)\n",
    "\n",
    "    h, w = featuresize\n",
    "    shift_x = np.arange(0, w) * scale # 基于特征图生成候选点网格\n",
    "    shift_y = np.arange(0, h) * scale\n",
    "    \n",
    "    # 通过基本anchor和基于特征图的候选点网格，生成整个特征图的anchor\n",
    "    anchor = []\n",
    "    for i in shift_y:\n",
    "        for j in shift_x:\n",
    "            anchor.append(base_anchor + [j, i, j, i])\n",
    "    return np.array(anchor).reshape((-1, 4)) # shape=(anchor_num, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交并比计算相关函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_iou(box1, box1_area, boxes2, boxes2_area):\n",
    "    \"\"\"\n",
    "    计算交并比 相交的面积和相并的面积的比值\n",
    "\n",
    "    box1 [x1,y1,x2,y2]            每个anchor\n",
    "    boxes2 [Msample,x1,y1,x2,y2]  所有的gtbox\n",
    "    \"\"\"\n",
    "    x1 = np.maximum(box1[0], boxes2[:, 0]) # 找出两个框靠左的横坐标的最大的那个 shape=(M,)\n",
    "    x2 = np.minimum(box1[2], boxes2[:, 2]) # 找出两个框靠右的横坐标的最小的那个 shape=(M,)\n",
    "    y1 = np.maximum(box1[1], boxes2[:, 1]) # 找出两个框靠上的纵坐标中最大的那个 shape=(M,)\n",
    "    y2 = np.minimum(box1[3], boxes2[:, 3]) # 找出两个框靠下的纵坐标中最小的那个 shape=(M,)\n",
    "\n",
    "    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0) # 如果x2-x1为负数或者y2-y1为负数，那么两个框是不相交的 shape=(M,)\n",
    "    iou = intersection / (box1_area + boxes2_area - intersection)  # 相交的面积和相并的面积的比值 shape=(M,)\n",
    "    return iou\n",
    "\n",
    "\n",
    "def cal_overlaps(boxes1, boxes2):\n",
    "    \"\"\"\n",
    "    计算出每个anchor分别和所有的gtbox的iou\n",
    "\n",
    "    boxes1 [Nsample,x1,y1,x2,y2]  anchor      shape=(N, 4)\n",
    "    boxes2 [Msample,x1,y1,x2,y2]  grouth-box  shape=(M, 4)\n",
    "    \"\"\"\n",
    "    area1 = (boxes1[:, 0] - boxes1[:, 2]) * (boxes1[:, 1] - boxes1[:, 3]) # shape=(N,)\n",
    "    area2 = (boxes2[:, 0] - boxes2[:, 2]) * (boxes2[:, 1] - boxes2[:, 3]) # shape=(M,)\n",
    "\n",
    "    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0])) # shape=(N, M)\n",
    "\n",
    "    for i in range(boxes1.shape[0]):\n",
    "        overlaps[i][:] = cal_iou(boxes1[i], area1[i], boxes2, area2) # cal_iou返回的shape=(M,) overlaps[i][:]的shape=(M,) 一次计算当前anchor和所有gtbox的iou\n",
    "\n",
    "    return overlaps # shape=(N, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把框的(x1, y1, x2, y2)坐标转换为(Vc， Vh)偏移量\n",
    "通过候选框和真实框计算出候选框需要怎么移动放大才能变成真实框，及反向计算  \n",
    "计算Vc为y坐标偏移量，Vh为框高度偏移量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_transfrom(anchors, gtboxes):\n",
    "    \"\"\"\n",
    "    通过候选框和真实框计算出候选框需要怎么移动放大才能变成真实框\n",
    "    计算Vc为y坐标偏移量，Vh为框高度偏移量\n",
    "    anchors shape=(N, 4)\n",
    "    gtboxes shape=(N, 4) 此时的gtboes是有anchor个，是每个anchor对应iou最大的那个gtbox\n",
    "    \"\"\"\n",
    "    cy  = (gtboxes[:, 1] + gtboxes[:, 3]) / 2.0 # shape=(N,)\n",
    "    cya = (anchors[:, 1] + anchors[:, 3]) / 2.0 # shape=(N,)\n",
    "    h   = gtboxes[:, 3] - gtboxes[:, 1] + 1.0 # shape=(N,)\n",
    "    ha  = anchors[:, 3] - anchors[:, 1] + 1.0 # shape=(N,)\n",
    "\n",
    "    vc = (cy - cya) / ha # shape=(N,)\n",
    "    vh = np.log(h / ha)  # shape=(N,)\n",
    "    \n",
    "    return np.vstack([vc, vh]).transpose() # shape=(2, N) transpose -> shape=(N, 2)\n",
    "\n",
    "\n",
    "def bbox_transfrom_inv(anchor, regr):\n",
    "    \"\"\"\n",
    "    通过基础anchor和预测出的偏移量，计算出预测出的bbox坐标\n",
    "    \"\"\"\n",
    "    cya = (anchor[:, 1] + anchor[:, 3]) / 2.0\n",
    "    ha  = (anchor[:, 3] - anchor[:, 1]) + 1\n",
    "\n",
    "    vc_pred = regr[0, :, 0]\n",
    "    vh_pred = regr[0, :, 1]\n",
    "\n",
    "    cy_pred = vc_pred * ha + cya\n",
    "    h_pred  = np.exp(vh_pred) * ha\n",
    "\n",
    "    cxa = (anchor[:, 0] + anchor[:, 2]) / 2.0\n",
    "\n",
    "    x1 = cxa - 16 / 2.0\n",
    "    y1 = cy_pred - h_pred / 2.0\n",
    "    x2 = cxa + 16 / 2.0\n",
    "    y2 = cy_pred + h_pred / 2.0\n",
    "    bbox = np.vstack([x1, y1, x2, y2]).transpose()\n",
    "\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算PRN网络的anchor标签  \n",
    "这个函数就是通过真实框和基础候选框计算出哪些候选框是前景哪些候选框是背景  \n",
    "这个就是rpn的核心  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_rpn(imgsize, featuresize, scale, gtboxes):\n",
    "    \"\"\"\n",
    "    计算anchor标签和anchor与gtbox的偏移量\n",
    "    就是制作RPN网络的标签\n",
    "    \"\"\"\n",
    "    imgh, imgw = imgsize\n",
    "\n",
    "    base_anchor = gen_anchor(featuresize, scale) # shape=(N, 4)\n",
    "\n",
    "    overlaps = cal_overlaps(base_anchor, gtboxes) # shape=(N, M)\n",
    "\n",
    "    labels = np.empty(base_anchor.shape[0]) # shape=(N,)\n",
    "    labels.fill(-1) # -1为忽略标签\n",
    "\n",
    "    gt_argmax_overlaps = overlaps.argmax(axis=0) # shape=(M,)\n",
    "\n",
    "    anchor_argmax_overlaps = overlaps.argmax(axis=1) # shape=(N,)\n",
    "    # 这里使用了numpy的坐标找值法，按shape的顺序输入坐标组，range(overlaps.shape[0])是选横坐标，anchor_argmax_overlaps选纵坐标\n",
    "    anchor_max_overlaps = overlaps[range(overlaps.shape[0]), anchor_argmax_overlaps] # shape=(N,)\n",
    "    \n",
    "    labels[anchor_max_overlaps > IOU_POSITIVE] = 1 # 给交并比大于阈值的anchor标签设置为1（前景）\n",
    "    labels[anchor_max_overlaps < IOU_NEGATIVE] = 0 # 给交并比小于与之的anchor标签设置为0（背景）\n",
    "    labels[gt_argmax_overlaps] = 1 # 各个gtbox交并比最大的anchor标签设置为1（前景），以保证每个gtbox至少有一个对应的anchor\n",
    "\n",
    "    outside_anchor = np.where(\n",
    "        (base_anchor[:, 0] < 0) |\n",
    "        (base_anchor[:, 1] < 0) |\n",
    "        (base_anchor[:, 2] >= imgw) |\n",
    "        (base_anchor[:, 3] >= imgh)\n",
    "        )[0]\n",
    "    labels[outside_anchor] = -1 # 把超出图片的anchor设置为忽略标签\n",
    "\n",
    "    # 抑制样本数量\n",
    "    # 抑制正样本数量\n",
    "    fg_index = np.where(labels == 1)[0] # 前景标签的下标\n",
    "    if(len(fg_index) > RPN_POSITIVE_NUM): # 如果选出的前景标签的数量大于设定默认前景标签数量，那么随机选出多余的部分设置值为-1\n",
    "        labels[np.random.choice(fg_index,len(fg_index) - RPN_POSITIVE_NUM, replace=False)] = -1\n",
    "\n",
    "    # 抑制负样本数量\n",
    "    bg_index = np.where(labels==0)[0] # 背景标签的下标\n",
    "    num_bg = RPN_TOTAL_NUM - np.sum(labels == 1) # 通过总有效候选框数量减去正数量得到应该有的背景数量\n",
    "    if(len(bg_index) > num_bg):\n",
    "        labels[np.random.choice(bg_index,len(bg_index) - num_bg, replace=False)] = -1 # 随机选择超出的背景标签设置为-1\n",
    "\n",
    "    # 把anchor坐标标签转换为Vc和Vh标签\n",
    "    bbox_targets = bbox_transfrom(base_anchor, gtboxes[anchor_argmax_overlaps, :]) \n",
    "    # gtboxes[anchor_argmax_overlaps,:]是找出对应anchor的IOU最大的gtbox\n",
    "\n",
    "    return [labels, bbox_targets], base_anchor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练数据生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 切分原始的gtbox\n",
    "因为CTPN的候选框是宽度固定的小框，也不会预测宽度和x坐标  \n",
    "所以需要先把真实框切分成等宽的小框以便计算rpn标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_gtboxes(gtboxes):\n",
    "    \"\"\"\n",
    "    把标签原始的大框，转换为宽度16像素的小框，以便后面计算anchor\n",
    "    \"\"\"\n",
    "    new_gtboxes = []\n",
    "    for gtbox in gtboxes:\n",
    "        x1 = gtbox[0]\n",
    "        y1 = gtbox[1]\n",
    "        x2 = gtbox[2]\n",
    "        y2 = gtbox[3]\n",
    "        gtbox_w = x2 - x1\n",
    "        w_steps = math.ceil(gtbox_w / 16)\n",
    "        for _ in range(w_steps):\n",
    "            new_gtboxes.append((x1, y1, x1+16, y2))\n",
    "            x1 += 16\n",
    "    \n",
    "    return np.array(new_gtboxes).reshape(-1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gtbox读取器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readtxt(path):\n",
    "    gtboxes = []\n",
    "    imgfile = os.path.splitext(os.path.split(path)[-1])[0].split('_')[-1] + '.jpg'\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = re.sub(r'[\"\\n]', '', line)\n",
    "            line = re.split(r'\\s', line)\n",
    "            gtboxes.append([int(line[0]), int(line[1]), int(line[2]), int(line[3])])\n",
    "    return np.asarray(gtboxes), imgfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataloader函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sample(textdir, imgdir):\n",
    "    \"\"\"\n",
    "    训练数据生成器\n",
    "    \"\"\"\n",
    "    textfiles = [file for file in os.listdir(textdir) if file.endswith('.txt')]\n",
    "    random.shuffle(textfiles)\n",
    "    textfiles = np.array(textfiles)\n",
    "\n",
    "    i = 0\n",
    "    end_index = len(textfiles)\n",
    "    \n",
    "    while True:\n",
    "        if i >= end_index:\n",
    "            i = 0\n",
    "        textfile = textfiles[i]\n",
    "        gtbox, imgfile = readtxt(os.path.join(textdir, textfile))\n",
    "        img = cv2.imread(os.path.join(imgdir, imgfile))\n",
    "        h, w, _ = img.shape\n",
    "\n",
    "        # 随机水平翻转\n",
    "        if np.random.randint(0, 100) > 50:\n",
    "            img = img[:, ::-1, :]\n",
    "            newx1 = w - gtbox[:, 2] - 1\n",
    "            newx2 = w - gtbox[:, 0] - 1\n",
    "            gtbox[:, 0] = newx1\n",
    "            gtbox[:, 2] = newx2\n",
    "\n",
    "        # 把大框切成小框\n",
    "        gtbox = split_gtboxes(gtbox)\n",
    "\n",
    "        # 计算出anchor标签\n",
    "        [clas, regr], _ = cal_rpn((h, w), (int(h/16), int(w/16)), 16, gtbox)\n",
    "\n",
    "        # 减去imagenet图像平均值，标准化图像\n",
    "        m_img = img - IMAGE_MEAN\n",
    "        m_img = m_img[np.newaxis, ...]           # shape=(1, h, w, c)\n",
    "\n",
    "        regr = np.hstack([clas.reshape(clas.shape[0], 1), regr])\n",
    "\n",
    "        clas = clas[np.newaxis, ..., np.newaxis] # shape=(1, HW10, 1) [1, HW10, [class]]\n",
    "        regr = regr[np.newaxis, ...]             # shape=(1, HW10, 3) [1, HW10, [class, Vc, Vh]]\n",
    "\n",
    "        yield m_img, {'rpn_class_reshape': clas, 'rpn_regress_reshape': regr}\n",
    "\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本线构造算法——小框合成大框"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测出来的是许多挨着的小框，需要把这些小框合并为大框\n",
    "1. 计算每个框的伙伴框(也就是和当前框相邻的框)\n",
    "2. 构建当前框和伙伴框的关系图表\n",
    "3. 通过关系图表合并小框到大框"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查找伙伴框"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 以当前i框为准，沿着x轴方向向后逐个像素查找k框  \n",
    "2. 对比每个找到的k框，是否y方向上的交并比在阈值内\n",
    "3. 如果交并比在阈值内，k框就为i框的伙伴框\n",
    "4. 返回k框在所有anchor中的index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meet_v_iou(index1, index2, text_proposals):\n",
    "    \"\"\"\n",
    "    用于检测两个框的垂直方向上的相似和重叠情况\n",
    "    \"\"\"\n",
    "    heights = text_proposals[:, 3] - text_proposals[:, 1] + 1\n",
    "\n",
    "    def overlaps_vertical(index1, index2):\n",
    "        \"\"\"\n",
    "        求两个框垂直方向上的重叠率\n",
    "        \"\"\"\n",
    "        h1 = heights[index1] # 框1的高\n",
    "        h2 = heights[index2] # 框2的高\n",
    "        # 两个框重叠处的y坐标\n",
    "        y0 = max(text_proposals[index2][1], text_proposals[index1][1]) \n",
    "        y1 = min(text_proposals[index2][3], text_proposals[index1][3])\n",
    "        return max(0, y1 - y0) / min(h1, h2) # 求重叠处的h处以两框中最短的h\n",
    "\n",
    "\n",
    "    def size_similarity(index1, index2):\n",
    "        \"\"\"\n",
    "        求两个框高度相似度\n",
    "        \"\"\"\n",
    "        h1 = heights[index1]\n",
    "        h2 = heights[index2]\n",
    "        return min(h1, h2) / max(h1, h2)\n",
    "\n",
    "    v_iou = overlaps_vertical(index1, index2) >= MIN_V_OVERLAPS and \\\n",
    "          size_similarity(index1, index2) >= MIN_SIZE_SIM\n",
    "\n",
    "    return v_iou # bool类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_successions(index, text_proposals, boxes_table, imgsize):\n",
    "    \"\"\"\n",
    "    查找当前的框向后有没有连续的候选框\n",
    "    找当前框向后的伙伴框\n",
    "    \"\"\"\n",
    "    _, w = imgsize\n",
    "\n",
    "    box = text_proposals[index] # 获取当前框\n",
    "    results = []\n",
    "    # 这里是开始在当前box右边x坐标之后，在gap个像素之内查找有没有候选框\n",
    "    for right in range(int(box[0]) + 1, min(int(box[0]) + MAX_HORIZONTAL_GAP, w)):\n",
    "        r_box_indices = boxes_table[right]\n",
    "        for r_box_index in r_box_indices:\n",
    "            # 比较右边的框和当前的框垂直方向上的相似度\n",
    "            # 这函数可以过滤掉x坐标相近但是y坐标相差很大的框，也就是跨行了\n",
    "            if meet_v_iou(r_box_index, index, text_proposals): \n",
    "                results.append(r_box_index)\n",
    "        if len(results) != 0:\n",
    "            return results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 验证伙伴框"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 以当前k框为准，沿着x轴方向向前逐个像素查找i框  \n",
    "2. 对比每个找到的i框，是否y方向上的交并比在阈值内\n",
    "3. 如果交并比在阈值内，i框就为k框的伙伴框\n",
    "4. 返回i框在所有anchor中的index\n",
    "5. 检查这里计算出的i框和之前选定的i框是否是同一个框\n",
    "6. 如果是同一个框那么可以确定i框和k框一定是伙伴框"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_successions(index, text_proposals, boxes_table, imgsize):\n",
    "    \"\"\"\n",
    "    查找当前的框向后有没有连续的候选框\n",
    "    找当前框向后的伙伴框\n",
    "    \"\"\"\n",
    "    _, w = imgsize\n",
    "\n",
    "    box = text_proposals[index] # 获取当前框\n",
    "    results = []\n",
    "    # 这里是开始在当前box右边x坐标之后，在gap个像素之内查找有没有候选框\n",
    "    for right in range(int(box[0]) + 1, min(int(box[0]) + MAX_HORIZONTAL_GAP, w)):\n",
    "        r_box_indices = boxes_table[right]\n",
    "        for r_box_index in r_box_indices:\n",
    "            # 比较右边的框和当前的框垂直方向上的相似度\n",
    "            # 这函数可以过滤掉x坐标相近但是y坐标相差很大的框，也就是跨行了\n",
    "            if meet_v_iou(r_box_index, index, text_proposals): \n",
    "                results.append(r_box_index)\n",
    "        if len(results) != 0:\n",
    "            return results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_succession_node(index, successions_index, text_proposals, scores, boxes_table):\n",
    "    \"\"\"\n",
    "    这个函数是以之前向后找出的框为基准再向前找符合要求的框\n",
    "    如果这个框的score比当前框小，那么当前框和向后的框是一个最长连接\n",
    "    这个应该是以分数为界，切割出一个个连接\n",
    "    \"\"\"\n",
    "    # 以向前查找到的j_index的框向前查找，找到k_index的框\n",
    "    precursors = get_precursors(successions_index, text_proposals, boxes_table) \n",
    "    # 如果scores_index > scores_k_index那么这个序列是最长连接\n",
    "    if scores[index] >= np.max(scores[precursors]): \n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建当前框和伙伴框的关系图表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 所有小框的左边x坐标为关键字，在x轴上映射一个转换表，这个表可以用像素x坐标查找n个像素内的框\n",
    "2. 查找伙伴框\n",
    "3. 通过验证的伙伴框会映射到伙伴关系图表中\n",
    "4. 多次循环前面2步构建完整的伙伴关系表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(text_proposals, scores, imgsize):\n",
    "    \"\"\"\n",
    "    构建框的伙伴关系图表\n",
    "    \"\"\"\n",
    "    _, w = imgsize\n",
    "\n",
    "    boxes_table = [[] for _ in range(w)]  # 以x轴的顺序建立一个表\n",
    "    for index, box in enumerate(text_proposals):\n",
    "        boxes_table[int(box[0])].append(index)  # 以小框左边x坐标，把小框映射到表中\n",
    "    \n",
    "    # 构造图表，形状为(N, N)第一维是当前框，第二维是伙伴框，伙伴框因该在当前框的右边\n",
    "    graph = np.zeros((text_proposals.shape[0], text_proposals.shape[0])) \n",
    "    \n",
    "    for index, box in enumerate(text_proposals):\n",
    "        successions = get_successions(index, text_proposals, boxes_table, imgsize)\n",
    "        if len(successions) == 0:\n",
    "            continue\n",
    "        # get_successions函数会找一组相邻的框，如果这一组有多个框这里选出的分最高的框\n",
    "        successions_index = successions[np.argmax(scores[successions])] \n",
    "        if is_succession_node(index, successions_index, text_proposals, scores, boxes_table):\n",
    "            # 确定伙伴关系，当index和successions_index的两个框是伙伴时，图表的这个坐标设置为True\n",
    "            graph[index, successions_index] = True \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过框的伙伴关系图表，合并小框到大框"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "伙伴关系图表：\n",
    "1. 假设一共有N个小框\n",
    "2. 伙伴关系图表M的形状为(N, N) 第一维是当前框的编号，第二维是伙伴框的编号\n",
    "3. 如果编号i的框有伙伴，那么M\\[i, :\\]有元素为True， 那为True的位置就i框伙伴的编号\n",
    "4. 如果编号k的框是其他框的伙伴，那么M\\[:, k\\]有元素为True，为True的位置编号的框的伙伴就是k框\n",
    "5. 如果s号框 M\\[:, s\\] 没有元素为True 且 M\\[s, :\\] 有元素为True，通过3、4点可以得出s号框不是任何框的伙伴，s号框有伙伴，所以s号框是一个大框的起始\n",
    "6. 如果e号框 M\\[:, e\\] 有元素为True 且 M\\[e, :\\] 没有元素为True，依然通过3、4点可以得出e号框是其他框的伙伴，e号框没有伙伴，所以e号框是一个大框的结尾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_graphs_connected(graph):\n",
    "    \"\"\"\n",
    "    通过框的伙伴关系图表，分出相连的框到一个列表中，这个列表中的小框是属于一个大框的\n",
    "    \"\"\"\n",
    "    sub_graphs = []\n",
    "    for index in range(graph.shape[0]):\n",
    "        # not graph[:, index].any() 这个是找的successions_index的框，如果这个框对应的那一列下没有True，那么说明这个坐标之前没有框和它连续\n",
    "        # graph[index, :].any() 找的是index的框，如果这个框对应的那一行下有True，那么说明这个坐标只有有和它连续的框\n",
    "        # 因此可以断定这个框是一个起始框\n",
    "        if not graph[:, index].any() and graph[index, :].any(): # 没有框和index连续，index有连续的框\n",
    "            v = index # 设置起始框的index\n",
    "            # 构建子框的列表，列表里面是存放小框的序号，这些小框是属于一个大框的\n",
    "            sub_graphs.append([v]) \n",
    "            while graph[v, :].any(): # v框有伙伴时\n",
    "                # np.where(graph[v, :])取出了那一行shape=(n,) \n",
    "                # 这句代码其实取出的是当前行的横坐标，也就是v对应伙伴的index\n",
    "                v = np.where(graph[v, :])[0][0]\n",
    "                # 把伙伴添加到当前子框列表中\n",
    "                sub_graphs[-1].append(v)\n",
    "                # 不断的迭代查找v有没有伙伴，有伙伴就继续查找伙伴的伙伴\n",
    "                # 这样最终找到所有连在一起的框\n",
    "    return sub_graphs # 返回的是嵌套列表，共两层\n",
    "                      # 外层是大框，内层是大框由那些小框组成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本线构造算法——计算大框的四个顶点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 通过合并框算法，得到大框列表L，L中的元素是小框的编号\n",
    "2. 拟合L中所有小框的左上点和右下点为一条直线\n",
    "3. 通过直线方程求解左上右上、左下右下4个点的坐标，得到大框的坐标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_y(X, Y, x1, x2):\n",
    "    \"\"\"\n",
    "    通过X和Y坐标集拟合一条直线\n",
    "    \"\"\"\n",
    "    # 这个句话翻译一下就是X中只有1个坐标，也就是X，Y点集只有一个点，这样就只能返回Y[0]坐标了\n",
    "    if np.sum(X == X[0]) == len(X): \n",
    "        return Y[0], Y[0]\n",
    "\n",
    "    line = np.poly1d(np.polyfit(X, Y, 1)) # 通过X，Y拟合一个一次方程也就是一条直线\n",
    "    \n",
    "    return line(x1), line(x2) # 返回x1,x2时这条直线上y坐标\n",
    "\n",
    "\n",
    "def threshold(coords, mini, maxi):\n",
    "    \"\"\"\n",
    "    把coords的大小压缩在mini和maxi之间\n",
    "    \"\"\"\n",
    "    return np.maximum(np.minimum(coords, maxi), mini)\n",
    "\n",
    "\n",
    "def clip_boxes(boxes, im_shape):\n",
    "    \"\"\"\n",
    "    Clip boxes to image boundaries.\n",
    "    剪切框，使框在图片中\n",
    "    \"\"\"\n",
    "    boxes[:, 0::2] = threshold(boxes[:, 0::2], 0, im_shape[1] - 1)\n",
    "    boxes[:, 1::2] = threshold(boxes[:, 1::2], 0, im_shape[0] - 1)\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 极大值抑制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于每个目标，预测出的框大多数情况有多个，极大值抑制就是在这些框中优选出最合适的框作为当前目标的预测框"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(dets, thresh):\n",
    "    \"\"\"\n",
    "    极大值抑制，过滤掉和最大值相近的框\n",
    "    这个算法是以每个框的得分从大到小来进行搜素需要被抑制的框\n",
    "    \"\"\"\n",
    "    x1 = dets[:, 0]\n",
    "    y1 = dets[:, 1]\n",
    "    x2 = dets[:, 2]\n",
    "    y2 = dets[:, 3]\n",
    "    scores = dets[:, 4]\n",
    "\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1) # shape=(N,)\n",
    "    order = scores.argsort()[::-1] # 从大到小的排序，返回的是index\n",
    "\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i) # 找出得分最大值\n",
    "        # 求解得分最大值和其他框的相交面积，求出交并比\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]]) # shape=(N-1,) 在order[1:]上寻找\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "        ovr = inter / (areas[i] + areas[order[1:]] - inter) # shape=(N-1,)\n",
    "        \n",
    "        # shape=(N-1)，找出低于阈值的框，因为高于阈值的框和当前得分最高的框重叠度很高\n",
    "        # 留下交并比小于阈值的框，交并比大于阈值的框被抑制了\n",
    "        inds = np.where(ovr <= thresh)[0] \n",
    "        # 出开了第一个也就是得分最大的那个, 选出的order依然是按照有大到小的顺序排列\n",
    "        # 继续从剩下的框中重复之前的计算，知道没有可用框\n",
    "        order = order[inds + 1] \n",
    "    \n",
    "    return keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练代码示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "from dataloaders import dataloader\n",
    "from models import cptn_base\n",
    "from keras.optimizers import Adam \n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from keras.backend.tensorflow_backend import set_session \n",
    "config = tf.ConfigProto() \n",
    "config.gpu_options.allow_growth=True\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "\n",
    "textdir = 'datasets/train_labels'\n",
    "imgdir  = 'datasets/train_images'\n",
    "\n",
    "train_gen = dataloader.gen_sample(textdir, imgdir)\n",
    "\n",
    "model = cptn_base.cptn_model((None, None, 3))\n",
    "model.compile(\n",
    "    optimizer=Adam(1e-5),\n",
    "    loss={'rpn_class_reshape': cptn_base.rpn_loss_clas, 'rpn_regress_reshape': cptn_base.rpn_loss_regr},\n",
    "    loss_weights={'rpn_class_reshape': 1.0, 'rpn_regress_reshape': 1.0}\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(r'weights/weights-ctpnlstm-{epoch:02d}.hdf5',\n",
    "                    save_weights_only=True)\n",
    "]\n",
    "\n",
    "model.fit_generator(\n",
    "    train_gen,\n",
    "    epochs=20,\n",
    "    steps_per_epoch=6000,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测代码示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from dataloaders import dataloader\n",
    "from models import cptn_base\n",
    "from keras.optimizers import Adam \n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "from dataloaders import dataloader, text_line_bulder\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "IMAGE_MEAN = [123.68,116.779,103.939]\n",
    "\n",
    "\n",
    "# 给训练网络加类别输出加一个激活函数，以便得到类别的评分\n",
    "model = cptn_base.cptn_model((None, None, 3))\n",
    "clas, regr = model.output\n",
    "input_tensor = model.input\n",
    "clas_prod = layers.Activation('softmax', name='rpn_cls_softmax')(clas)\n",
    "\n",
    "predict_model = Model(input_tensor, [clas, regr, clas_prod])\n",
    "predict_model.load_weights('weights/weights-ctpnlstm-20_20190412_0817.hdf5')\n",
    "\n",
    "# test_img = cv2.imread('datasets/train_images/100.jpg')\n",
    "test_img = cv2.imread('/home/y/文档/神经网络数据集/Challenge2_Test_Task12_Images/img_14.jpg')\n",
    "h, w, c = test_img.shape\n",
    "pred_img = test_img - IMAGE_MEAN\n",
    "pred_img = pred_img[np.newaxis, ...]\n",
    "\n",
    "# 预测\n",
    "clas, regr, clas_pord = predict_model.predict(pred_img)\n",
    "\n",
    "# 通过基本anchor和预测的regr还原预测的bbox\n",
    "anchor = dataloader.gen_anchor((int(h / 16), int(w / 16)), 16)\n",
    "bbox = dataloader.bbox_transfrom_inv(anchor, regr)\n",
    "bbox = dataloader.clip_box(bbox, [h, w])\n",
    "\n",
    "# 选出类别评分大于0.7的框\n",
    "fg = np.where(clas_pord[0, :, 1] > 0.7)[0]\n",
    "select_anchor = bbox[fg, :]\n",
    "select_score = clas_pord[0, fg, 1]\n",
    "select_anchor = select_anchor.astype('int32')\n",
    "\n",
    "# 过滤掉宽和高小于阈值的框\n",
    "keep_index = dataloader.filter_bbox(select_anchor, 16)\n",
    "select_anchor = select_anchor[keep_index]\n",
    "select_score  = select_score[keep_index]\n",
    "\n",
    "# 极大值抑制\n",
    "select_score = np.reshape(select_score, (select_score.shape[0], 1)) # shape=(N,) -> shape=(N, 1)\n",
    "nmsbox = np.hstack([select_anchor, select_score]) # shape=(N, 5) [N, [x1, y1, x2, y2, score]]\n",
    "keep = dataloader.nms(nmsbox, 0.3) # 计算出留下的anchor的下标\n",
    "select_anchor = select_anchor[keep]\n",
    "select_score  = select_score[keep]\n",
    "\n",
    "# 使用文本线构造算法，把检测出的小框合并为大框\n",
    "text_recs = text_line_bulder.get_text_lines(select_anchor, select_score, (h, w))\n",
    "text_recs = text_recs.astype('int32')\n",
    "\n",
    "for i in text_recs:\n",
    "    cv2.line(test_img, (i[0], i[1]), (i[2], i[3]), (255, 0, 0), 2)\n",
    "    cv2.line(test_img, (i[0], i[1]), (i[4], i[5]), (255, 0, 0), 2)\n",
    "    cv2.line(test_img, (i[6], i[7]), (i[2], i[3]), (255, 0, 0), 2)\n",
    "    cv2.line(test_img, (i[4], i[5]), (i[6], i[7]), (255, 0, 0), 2)\n",
    "\n",
    "# for i in select_anchor:\n",
    "#     cv2.rectangle(test_img, (i[0], i[1]), (i[2], i[3]), (0, 255, 0))\n",
    "\n",
    "plt.imshow(test_img[..., ::-1])\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
